{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('rl')",
   "display_name": "Python 3.8.6 64-bit ('rl')",
   "metadata": {
    "interpreter": {
     "hash": "8f2b65e0f307dab7425f28ae6e0a2df90287e9beb30a8401009830eabbcdbb50"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer():\n",
    "    def __init__(self, size, obs_dim, act_dim, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.max_size = size\n",
    "        self.ptr = 0\n",
    "        self.traj_start_idx = 0\n",
    "\n",
    "\n",
    "    def _discount_cumsum(self, x, discount):\n",
    "        \"\"\"\n",
    "        The code below calculates the cummulative discounted sum.\n",
    "        A more efficient way of doing it, but less readible is the following:\n",
    "            return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "        \"\"\"\n",
    "        cumsum = np.zeros(len(x), dtype=np.float32)\n",
    "        cumsum[-1] = x[-1]\n",
    "        for i in range(len(x) - 2, -1, -1):\n",
    "            cumsum[i] = x[i] + discount * cumsum[i+1]\n",
    "        return cumsum\n",
    "        \n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size # there must be space in the buffer to store\n",
    "\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "\n",
    "    def end_trajectory(self, last_val):\n",
    "        traj_slice = slice(self.traj_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[traj_slice], last_val)\n",
    "        vals = np.append(self.val_buf[traj_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[traj_slice] = self._discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        # the next line computes the reward to go\n",
    "        self.ret_buf[traj_slice] = self._discount_cumsum(rews, self.gamma)[:-1]\n",
    "\n",
    "        self.traj_start_idx = self.ptr\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size # buffer must be full\n",
    "\n",
    "        # reset the buffer\n",
    "        self.ptr, self.traj_start_idx = 0, 0\n",
    "\n",
    "        # normalize advantages for training stability\n",
    "        adv_mean = np.mean(self.adv_buf)\n",
    "        adv_std = np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, adv=self.adv_buf, logp=self.logp_buf)\n",
    "\n",
    "        # convert data to dict of torch tensors\n",
    "        data = {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Linear(obs_dim, 100))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Linear(100, 50))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Linear(50, act_dim))\n",
    "\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    \n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Linear(obs_dim, 100))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Linear(100, 50))\n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(nn.Linear(50, 1))\n",
    "        self.v_net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.v = Critic(obs_dim)\n",
    "        self.pi = Actor(obs_dim, act_dim)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGAgent():\n",
    "    def __init__(self, obs_dim, act_dim, gamma=0.99, pi_lr=5e-3, vf_lr=5e-3, lam=0.97, train_v_iters=20, steps_per_epoch=4000):\n",
    "        \n",
    "        self.buf = VPGBuffer(steps_per_epoch, obs_dim, act_dim, gamma, lam)\n",
    "\n",
    "        self.ac = ActorCritic(obs_dim, act_dim)\n",
    "\n",
    "        # Set up optimizers for policy and value function\n",
    "        self.pi_optimizer = optim.Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = optim.Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.train_v_iters = train_v_iters\n",
    "\n",
    "\n",
    "    def _compute_loss_pi(self, data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = self.ac.pi(obs, act)\n",
    "        loss_pi = -(logp * adv).mean() # negative log probability loss\n",
    "\n",
    "        return loss_pi\n",
    "\n",
    "\n",
    "    def _compute_loss_v(self, data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((self.ac.v(obs) - ret)**2).mean() # MSE  \n",
    "\n",
    "    def update(self):\n",
    "        data = self.buf.get()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        self.pi_optimizer.zero_grad()\n",
    "        loss_pi = self._compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        self.pi_optimizer.step()\n",
    "\n",
    "        for i in range(self.train_v_iters):\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            loss_v = self._compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            self.vf_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env_name, epochs, max_ep_len=1000):\n",
    "    ep_ret, ep_len = 0, 0\n",
    "    ep_rets = []\n",
    "    for epoch in range(epochs):\n",
    "        env = gym.make(env_name)\n",
    "        o = env.reset()\n",
    "        for t in range(agent.steps_per_epoch):\n",
    "            a, v, logp = agent.ac.step(torch.as_tensor(np.ascontiguousarray(o), dtype=torch.float32).unsqueeze(0))\n",
    "            a = a.squeeze(0)\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            agent.buf.store(o, a, r, v, logp)\n",
    "\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t == agent.steps_per_epoch - 1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = agent.ac.step(torch.as_tensor(np.ascontiguousarray(o), dtype=torch.float32).unsqueeze(0))\n",
    "                else:\n",
    "                    v = 0\n",
    "                agent.buf.end_trajectory(v)\n",
    "                ep_rets.append(ep_ret)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        agent.update()\n",
    "        env.close()\n",
    "\n",
    "        print('Epoch: ', epoch,'avg ep_ret: ', np.mean(ep_rets[-10:]), \"total num ep: \", len(ep_rets))\n",
    "\n",
    "    return ep_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "agent = VPGAgent(8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  0 avg ep_ret:  -12.661636028556584 total num ep:  4\n",
      "Epoch:  1 avg ep_ret:  -23.301186403458004 total num ep:  8\n",
      "Epoch:  2 avg ep_ret:  -29.85880525216813 total num ep:  12\n",
      "Epoch:  3 avg ep_ret:  -40.45664762965329 total num ep:  16\n",
      "Epoch:  4 avg ep_ret:  -55.245811158687125 total num ep:  20\n",
      "Epoch:  5 avg ep_ret:  -62.62520405030059 total num ep:  24\n",
      "Epoch:  6 avg ep_ret:  -69.40350972282134 total num ep:  28\n",
      "Epoch:  7 avg ep_ret:  -80.11731962728348 total num ep:  32\n",
      "Epoch:  8 avg ep_ret:  -69.44061181047671 total num ep:  36\n",
      "Epoch:  9 avg ep_ret:  -63.89876254908053 total num ep:  40\n",
      "Epoch:  10 avg ep_ret:  -47.195403902971286 total num ep:  45\n",
      "Epoch:  11 avg ep_ret:  -23.377740323343282 total num ep:  49\n",
      "Epoch:  12 avg ep_ret:  -15.229735336747684 total num ep:  54\n",
      "Epoch:  13 avg ep_ret:  -37.35841291337812 total num ep:  60\n",
      "Epoch:  14 avg ep_ret:  -40.33778022155981 total num ep:  66\n",
      "Epoch:  15 avg ep_ret:  -53.27992673013862 total num ep:  75\n",
      "Epoch:  16 avg ep_ret:  -81.10607265742549 total num ep:  85\n",
      "Epoch:  17 avg ep_ret:  -98.30788986008073 total num ep:  99\n",
      "Epoch:  18 avg ep_ret:  -95.66703273547026 total num ep:  112\n",
      "Epoch:  19 avg ep_ret:  -83.82462076446845 total num ep:  124\n",
      "Epoch:  20 avg ep_ret:  -54.63990629151598 total num ep:  131\n",
      "Epoch:  21 avg ep_ret:  -74.51568330022188 total num ep:  136\n",
      "Epoch:  22 avg ep_ret:  -59.09861827173294 total num ep:  141\n",
      "Epoch:  23 avg ep_ret:  -45.091343857225155 total num ep:  145\n",
      "Epoch:  24 avg ep_ret:  -46.36271603253626 total num ep:  149\n",
      "Epoch:  25 avg ep_ret:  -21.63268588409045 total num ep:  154\n",
      "Epoch:  26 avg ep_ret:  -36.51284438731617 total num ep:  158\n",
      "Epoch:  27 avg ep_ret:  -24.595014961232124 total num ep:  163\n",
      "Epoch:  28 avg ep_ret:  -16.93647268605847 total num ep:  167\n",
      "Epoch:  29 avg ep_ret:  -17.18783171403307 total num ep:  171\n",
      "Epoch:  30 avg ep_ret:  -7.999415813701063 total num ep:  175\n",
      "Epoch:  31 avg ep_ret:  -7.8653873342095295 total num ep:  179\n",
      "Epoch:  32 avg ep_ret:  4.759906248366435 total num ep:  183\n",
      "Epoch:  33 avg ep_ret:  31.837294941140307 total num ep:  187\n",
      "Epoch:  34 avg ep_ret:  54.03919916346685 total num ep:  191\n",
      "Epoch:  35 avg ep_ret:  66.68163049489465 total num ep:  196\n",
      "Epoch:  36 avg ep_ret:  87.47622979847499 total num ep:  202\n",
      "Epoch:  37 avg ep_ret:  77.5815822277328 total num ep:  207\n",
      "Epoch:  38 avg ep_ret:  26.49580898417496 total num ep:  213\n",
      "Epoch:  39 avg ep_ret:  52.43205860623558 total num ep:  219\n",
      "Epoch:  40 avg ep_ret:  43.54459434894088 total num ep:  224\n",
      "Epoch:  41 avg ep_ret:  9.477744243026361 total num ep:  229\n",
      "Epoch:  42 avg ep_ret:  -11.52594373766825 total num ep:  233\n",
      "Epoch:  43 avg ep_ret:  -33.35594596283843 total num ep:  238\n",
      "Epoch:  44 avg ep_ret:  -53.974094884768 total num ep:  242\n",
      "Epoch:  45 avg ep_ret:  -51.285855770927114 total num ep:  246\n",
      "Epoch:  46 avg ep_ret:  -55.204444618085574 total num ep:  250\n",
      "Epoch:  47 avg ep_ret:  -56.86505954578239 total num ep:  254\n",
      "Epoch:  48 avg ep_ret:  -57.49264768666704 total num ep:  258\n",
      "Epoch:  49 avg ep_ret:  -63.26934358349466 total num ep:  262\n",
      "Epoch:  50 avg ep_ret:  -63.51380813592277 total num ep:  267\n",
      "Epoch:  51 avg ep_ret:  -60.11284027450889 total num ep:  272\n",
      "Epoch:  52 avg ep_ret:  -42.598945723464105 total num ep:  277\n",
      "Epoch:  53 avg ep_ret:  -23.971262402250296 total num ep:  281\n",
      "Epoch:  54 avg ep_ret:  -39.590566839010656 total num ep:  285\n",
      "Epoch:  55 avg ep_ret:  -54.09729920713729 total num ep:  290\n",
      "Epoch:  56 avg ep_ret:  -35.63894695822957 total num ep:  295\n",
      "Epoch:  57 avg ep_ret:  -26.5600919296074 total num ep:  299\n",
      "Epoch:  58 avg ep_ret:  -16.253425990445102 total num ep:  304\n",
      "Epoch:  59 avg ep_ret:  -21.56723847676269 total num ep:  309\n",
      "Epoch:  60 avg ep_ret:  -17.92152331501932 total num ep:  314\n",
      "Epoch:  61 avg ep_ret:  -1.6956351272649186 total num ep:  319\n",
      "Epoch:  62 avg ep_ret:  -11.009202808009793 total num ep:  323\n",
      "Epoch:  63 avg ep_ret:  -4.7461862380408775 total num ep:  327\n",
      "Epoch:  64 avg ep_ret:  -30.747780065604577 total num ep:  331\n",
      "Epoch:  65 avg ep_ret:  -29.07062908482332 total num ep:  335\n",
      "Epoch:  66 avg ep_ret:  -21.005573337523614 total num ep:  339\n",
      "Epoch:  67 avg ep_ret:  13.190288141171493 total num ep:  344\n",
      "Epoch:  68 avg ep_ret:  31.337699535531687 total num ep:  349\n",
      "Epoch:  69 avg ep_ret:  28.54290325368799 total num ep:  353\n",
      "Epoch:  70 avg ep_ret:  82.8309652037506 total num ep:  359\n",
      "Epoch:  71 avg ep_ret:  93.70637232724836 total num ep:  364\n",
      "Epoch:  72 avg ep_ret:  93.68421873423296 total num ep:  369\n",
      "Epoch:  73 avg ep_ret:  107.91136799651122 total num ep:  374\n",
      "Epoch:  74 avg ep_ret:  112.1752263719122 total num ep:  379\n",
      "Epoch:  75 avg ep_ret:  136.9053381843377 total num ep:  384\n",
      "Epoch:  76 avg ep_ret:  133.07841726531223 total num ep:  390\n",
      "Epoch:  77 avg ep_ret:  135.02396314331418 total num ep:  396\n",
      "Epoch:  78 avg ep_ret:  133.45898173943004 total num ep:  402\n",
      "Epoch:  79 avg ep_ret:  119.7369462333348 total num ep:  407\n",
      "Epoch:  80 avg ep_ret:  103.0102382032092 total num ep:  412\n",
      "Epoch:  81 avg ep_ret:  102.94814971543576 total num ep:  417\n",
      "Epoch:  82 avg ep_ret:  105.38289523914105 total num ep:  422\n",
      "Epoch:  83 avg ep_ret:  119.20191342086173 total num ep:  428\n",
      "Epoch:  84 avg ep_ret:  129.47584441242464 total num ep:  433\n",
      "Epoch:  85 avg ep_ret:  144.62493595968118 total num ep:  439\n",
      "Epoch:  86 avg ep_ret:  144.95526358103555 total num ep:  445\n",
      "Epoch:  87 avg ep_ret:  140.53452628236454 total num ep:  451\n",
      "Epoch:  88 avg ep_ret:  136.96439256074447 total num ep:  458\n",
      "Epoch:  89 avg ep_ret:  151.36886898364403 total num ep:  464\n",
      "Epoch:  90 avg ep_ret:  157.80832584558507 total num ep:  469\n",
      "Epoch:  91 avg ep_ret:  160.6344435631193 total num ep:  474\n",
      "Epoch:  92 avg ep_ret:  151.46904271593453 total num ep:  478\n",
      "Epoch:  93 avg ep_ret:  156.1670706568247 total num ep:  482\n",
      "Epoch:  94 avg ep_ret:  153.7502377925117 total num ep:  486\n",
      "Epoch:  95 avg ep_ret:  137.9728733236451 total num ep:  490\n",
      "Epoch:  96 avg ep_ret:  147.22081946167776 total num ep:  495\n",
      "Epoch:  97 avg ep_ret:  173.38800829033124 total num ep:  500\n",
      "Epoch:  98 avg ep_ret:  171.29915334023613 total num ep:  505\n",
      "Epoch:  99 avg ep_ret:  158.15651272319604 total num ep:  510\n"
     ]
    }
   ],
   "source": [
    "rets = train(agent, env_name, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env_name, num_games=5):\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    def step(env, act):\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        env.render()\n",
    "        return obs, rew, done, _\n",
    "\n",
    "    for game in range(num_games):\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            act = agent.ac.act(torch.tensor(obs).unsqueeze(0))\n",
    "            act = act.squeeze(0)\n",
    "            obs, rew, done, _ = step(env, act)\n",
    "            \n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(agent, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}